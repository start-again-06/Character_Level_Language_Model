# ğŸ” RNN, LSTM, and Character-Level Language Modeling

This repository includes:
- NumPy-based implementation of RNN and LSTM (forward & backward passes)
- Character-level language generation (trained on dinosaur names)
- Sampling and optimization loops for text generation

---

## ğŸ§  Architecture Overview

### ğŸ”· RNN
- Hidden state propagation using tanh
- Forward and backward pass implemented manually
- Gradient clipping and parameter updates

### ğŸ”¶ LSTM
- Four gates: Forget, Input, Candidate, Output
- More stable gradient flow through long sequences
- Full Backpropagation Through Time (BPTT) supported

---

## ğŸ“š Character-Level Language Modeling

- Reads input text (e.g. `dinos.txt`)
- Creates character vocabulary and mappings
- Trains RNN to predict next character
- Generates new sequences by sampling from softmax distribution

---
## âš™ï¸ Key Components

### ğŸ”¸ Forward Pass
- `rnn_cell_forward()` â€“ Forward pass for one RNN cell
- `rnn_forward()` â€“ Loops through sequence for RNN
- `lstm_cell_forward()` â€“ One-step forward pass for LSTM
- `lstm_forward()` â€“ Full LSTM forward pass across sequence

### ğŸ”¸ Backward Pass
- `rnn_cell_backward()` â€“ Computes gradients for one RNN step
- `rnn_backward()` â€“ Aggregates gradients for RNN
- `lstm_cell_backward()` â€“ Computes gate derivatives for one step of LSTM
- `lstm_backward()` â€“ Full BPTT over LSTM

### ğŸ§ª Utilities
- `clip()` â€“ Gradient clipping to avoid exploding gradients
- `sample()` â€“ Random sampling from softmax probabilities
- `optimize()` â€“ Forward, backward and update steps in training
- `model()` â€“ Training loop for generating character-level names

---

## ğŸ“‚ File Structure

```bash
â”œâ”€â”€ rnn_utils.py         # Activation functions, initializations
â”œâ”€â”€ rnn_lstm_main.py     # Full RNN & LSTM logic
â”œâ”€â”€ dinos.txt            # Input data for language modeling
â”œâ”€â”€ utils.py             # Sampling, optimization, gradient clipping
```

---

## ğŸ”¬ Sample Outputs

```text
a[4][3][6] = 0.2197
c[1][2][1] = -0.2219
gradients["dWax"].shape = (5,3)
gradients["dWc"].shape = (5,8)
list of sampled characters: ['t', 'o', 'r', 'a', 'n', 'o', 'z', 'a', 'u', 'r']
```

---

## ğŸ” Training Loop Example

```python
parameters = model(data, ix_to_char, char_to_ix, num_iterations=10000)
```

- Generates new dinosaur names every 2000 iterations
- Smooths loss with exponential moving average
- Resets and shuffles input sequences for each epoch

---
